{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Assigment 1\n",
    "\n",
    "For this assignment you will use the following SVM implementation for classifying these datasets:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
    "\n",
    "You should:\n",
    "\n",
    "1) Specify which Machine Learning problem are you solving.\n",
    "\n",
    "2) Provide a short summary of the features and the labels you are working on.\n",
    "\n",
    "3) Please answer the following questions: a) Are these datasets linearly separable? b) Are these datasets randomly chosen and c) The sample size is enough to guarantee generalization.\n",
    "\n",
    "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
    "\n",
    "5) Show some examples to illustrate that the method is working properly.\n",
    "\n",
    "6) Provide quantitative evidence for generalization using the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from models import  train_validation_test_split,SVM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank note model \n",
    "\n",
    "1) Specify which Machine Learning problem are you solving.\n",
    "\n",
    "2) Provide a short summary of the features and the labels you are working on.\n",
    "\n",
    "3. Please answer the following questions: \n",
    "    1. Are these datasets linearly separable? \n",
    "    2. Are these datasets randomly chosen.\n",
    "    3. The sample size is enough to guarantee generalization.\n",
    "\n",
    "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
    "\n",
    "5) Show some examples to illustrate that the method is working properly.\n",
    "\n",
    "6) Provide quantitative evidence for generalization using the provided dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por hacer \n",
    "\n",
    "* Investigar más sobre Curtosis \n",
    "* Punto 4\n",
    "* Punto 5\n",
    "\n",
    "Segundo dataset\n",
    "\n",
    "* Punto 1\n",
    "* Punto 2\n",
    "* Punto 3\n",
    "* Punto 4\n",
    "* Punto 5 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Specify which Machine Learning problem are you solving.\n",
    "The machine learning problem in this case is to detect counterfeit banknotes, the data is obtained from https://archive.ics.uci.edu/ml/datasets/banknote+authentication , in this case we are in front of a classification problem,  where we try to classify each banknote into two possible classes, counterfeits and genuine banknotes, the objective is to maximize the detection of counterfeit banknotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_bank_note = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt', header=None)\n",
    "\n",
    "df_bank_note.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_note['class']=df_bank_note['class'].replace(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_note=shuffle(df_bank_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_note['class']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In based on this data,in effect, variance, skewness, curtosis and entropy we want to determine the class of every banknote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_bank_note['class']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_bank_note['class']==1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2  Provide a short summary of the features and the labels you are working on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For create this dataset was required to extract the features from images of genuine and forged banknote-like specimens, to do this they use wavelet transformation tool.we are going to introduce briefly the wavelet transformation tool,this transformations works similar to a Fourier tranformation, with the difference that the functions used are wavelets, that is functions of the form:\n",
    "\n",
    "$$ \\psi = -(x-b)e^{\\frac{-(x-b)^{2}/2a²}{\\sqrt{2\\pi}a^{3}}} $$\n",
    "\n",
    "Were $a$ determines scale and $b$ determines location,the wavelets have the advantage of extract local information , that is, if we want a better model for local information, wavelet transform work better than Fourier tranform, in this case since the genuine banknotes have local information to proof that is in fact genuine (like watermarks and othes markers) this information is so relevant, that is the reason for use wavelet transformation insted of Fourier or other transform,the wavelet transform in the  from this transformation the features want to describe the wavelets distributions,following this reasoning we obtain.\n",
    "\n",
    "1. variance of Wavelet Transformed image (continuous):\n",
    "    Over the distribution of wavelets transformation, we want to know his variance, the variance is a interesting value for our problem because a forged banknote probably have diferent ranges that a genuine.\n",
    "2. Skewness of Wavelet Transformed image (continuous):\n",
    "    The skewness give us a measure of the simetry of the distribution, we can think that exists a correlation between a class and his simetry of distribution.\n",
    "3. Curtosis of Wavelet Transformed image (continuous)\n",
    "    Curtosis is a measure of the density of the distribution,that give us information about the distribution of the pixels and can describe a principal element of the banknote.\n",
    "    \n",
    "4. Entropy of image (continuous)\n",
    "    If we work with pixels like a random variable, the entropy show us the \"disorder\" or uncertainty in this random variable, the use of specific materials and inks determine if a banknote are genuine or not, this can be characterised with the entropy, because the behaviour of the pixels with some materials can be defined by his entropy.  \n",
    "5. class (integer)\n",
    "    In this case we have two categories, the dataset description do not have the meaning of the class, but we could think that 0 is genuine and 1 is forged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_note.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe the different statistics measures of the data as a summary of principal statistical information of every variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Answer questions\n",
    "\n",
    "* ¿Are this dataset linearly separable?\n",
    "\n",
    "To see that this dataset are linearly separable we have to find a separator hiperplane who induce two partitions, in both cases we only need to find the support vector machine, if we can find it, then the data are linearly separable. But we could also run a clustering algorithm to see that we could separate the set of points in two collections, we are going to run a logistic regression algorithm to see that the data are separable and after a SVM algorithm.Also we are going to visualize the dataset and see if the two datasets looks like if were separables,in fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing data\n",
    "\n",
    "pair=sns.pairplot(df_bank_note, hue=\"class\")\n",
    "pair.fig.subplots_adjust(top=.93)\n",
    "pair.fig.suptitle('Marginal plot and distributions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distributions of de data in both cases looks different and distinguible one for another.And also we can see a correlation matrix to see that the data are related to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"dark\")\n",
    "corr = df_bank_note.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool)).T\n",
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "ax.set_title(\"Correlation Matrix\")\n",
    "# Generate a custom diverging colormap\n",
    "# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, center=0,cmap='coolwarm',\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =df_bank_note.drop('class',axis=1).values\n",
    "y= df_bank_note['class'].values\n",
    "\n",
    "# Scale the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test= train_validation_test_split(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to see if the data are linearly separable with a logistic regression over all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=0,max_iter=10000)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information we now see that a model can separate with an low error the dataset in to classes. And with this we can say that are linearly separable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Are this dataset randomly chosen?\n",
    "\n",
    "The function  *train_validation_test_split* is designed to randomly choose the data and split in 3 sets, i.e, train, test and validation, we can see that the distribution of the data is preserved, to do this we plot the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair=sns.pairplot(pd.DataFrame(X_train))\n",
    "pair.fig.subplots_adjust(top=.93)\n",
    "pair.fig.suptitle('Marginal plot and distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The sample size is $754$ and the dimension of every observation are $4$ , the dimesion $VC$ of a support vector machine is $d+1$ , in this case is $5$ then we can use an heuristics that if $N$ fulfil that $10d_{vc}(\\mathcal{H}\n",
    ")\\leq N$ then the model can generalize. In this case $50\\leq N$ and for this reason we can give some guarantee of generalization.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4  Provide an explanation how and why the code is working.\n",
    "\n",
    "To know more about the implementation of the support vector machine, the algorithm of optimization and more, you can check the file *models.py* where are the information and the commented code, in summary we create an object with name *SVM* , now we are going to implement the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train, y_train,X_val,y_val,learning_rate=0.001,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.plot_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.plot_omega()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Show some examples to illustrate that the method is working properly.\n",
    "\n",
    "To do this we create a function with the scaler and the model, in this case we insert the data and see how this works with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(data):\n",
    "    X=data.drop('class',axis=1).values\n",
    "    X=scaler.transform(X)\n",
    "    return svm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_bank_note['class']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data=df_bank_note[750:770]\n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(svm_model(example_data)==example_data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_note.drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos=scaler.transform(df_bank_note.drop('class',axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.w[0][0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(svm.predict(datos))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupancy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupancy model \n",
    "\n",
    "1) Specify which Machine Learning problem are you solving.\n",
    "\n",
    "2) Provide a short summary of the features and the labels you are working on.\n",
    "\n",
    "3. Please answer the following questions: \n",
    "    1. Are these datasets linearly separable? \n",
    "    2. Are these datasets randomly chosen and \n",
    "    3. The sample size is enough to guarantee generalization.\n",
    "\n",
    "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
    "\n",
    "5) Show some examples to illustrate that the method is working properly.\n",
    "\n",
    "6) Provide quantitative evidence for generalization using the provided dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occupancy = pd.read_csv('occupancy_data/datatraining.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir la columna de fechas a un objeto datetime\n",
    "df_occupancy['date'] = pd.to_datetime(df_occupancy['date'])\n",
    "\n",
    "# establecer la fecha de referencia\n",
    "reference_date = pd.to_datetime('2015-02-04')\n",
    "\n",
    "# convertir fechas a segundos desde la fecha de referencia\n",
    "df_occupancy['date_seconds'] = (df_occupancy['date'] - reference_date).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axs = plt.subplots(2,3,figsize=(16,16))\n",
    "\n",
    "count=0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        print(count)\n",
    "        column=df_occupancy.columns[count]\n",
    "        axs[i,j].hist(df_occupancy[column],rwidth=0.7)\n",
    "        print(column)\n",
    "        axs[i,j].set_title(f'Histogram of {column}')\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =df_occupancy.drop(['Occupancy','date'],axis=1).values\n",
    "y= df_occupancy['Occupancy'].replace(0,-1)\n",
    "y= y.values\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occupancy.drop(['Occupancy','date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing data\n",
    "\n",
    "plt.scatter(df_occupancy['Humidity'],df_occupancy['CO2'],alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"dark\")\n",
    "corr = df_occupancy.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool)).T\n",
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "ax.set_title(\"Correlation Matrix\")\n",
    "# Generate a custom diverging colormap\n",
    "# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_occupancy = SVM()\n",
    "history = svm_occupancy.fit(X_train, y_train,learning_rate=0.001,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_occupancy.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c38430d1cb24406c60f855c4378206f9200d970d482ac302cd5a44ba0f9ead88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
